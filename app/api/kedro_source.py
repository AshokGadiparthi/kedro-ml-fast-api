"""
Kedro Source Code Download API
==============================
Clone the Kedro ML engine from GitHub, patch catalog.yml and parameters.yml
based on single-table or multi-table (collection) mode, return as zip.

Endpoints:
  Single table (no collection_id):
    GET /api/v1/kedro-source/download?project_id=abc&file_path=data/01_raw/abc/data.csv

  Multi table (with collection_id):
    GET /api/v1/kedro-source/download?project_id=abc&file_path=data/01_raw/abc/m1/application_train.csv&collection_id=e273...

Flow:
  1. Clone repo from GitHub
  2. Patch catalog.yml  â†’ raw_data.filepath = file_path
  3. If collection_id:
     - Fetch collection metadata (tables, relationships, aggregations) from DB
     - Generate multi-table data_loading section
     - Replace parameters.yml with multi-table version
  4. Zip & return
"""

import json
import os
import re
import shutil
import subprocess
import tempfile
import logging
from pathlib import Path
from typing import Optional

import yaml

from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException, Query
from fastapi.responses import FileResponse
from sqlalchemy.orm import Session

from app.core.database import get_db
from app.models.models import (
    DatasetCollection, CollectionTable, TableRelationship, TableAggregation,
)

logger = logging.getLogger(__name__)
router = APIRouter()

GITHUB_REPO_URL = "https://github.com/AshokGadiparthi/kedro-ml-engine-integrated.git"
CATALOG_REL_PATH = "conf/base/catalog.yml"
PARAMS_REL_PATH = "conf/base/parameters.yml"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CATALOG PATCHING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _patch_catalog(catalog_path: Path, raw_data_filepath: str) -> None:
    """
    Replace raw_data filepath in catalog.yml.

    BEFORE:  filepath: data/01_raw/data.csv
    AFTER:   filepath: <raw_data_filepath>
    """
    text = catalog_path.read_text(encoding="utf-8")

    pattern = r"(raw_data:\s*\n\s*type:\s*[^\n]+\n\s*filepath:\s*)([^\n]+)"
    patched, count = re.subn(pattern, rf"\g<1>{raw_data_filepath}", text, count=1)

    if count == 0:
        pattern2 = r"(filepath:\s*)data/01_raw/[^\n]+"
        patched, count = re.subn(pattern2, rf"\g<1>{raw_data_filepath}", text, count=1)

    if count == 0:
        raise ValueError(f"Could not find raw_data filepath in {catalog_path}")

    catalog_path.write_text(patched, encoding="utf-8")
    logger.info(f"âœ… Patched catalog.yml: raw_data.filepath â†’ {raw_data_filepath}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MULTI-TABLE: Collection â†’ data_loading YAML
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _auto_generate_aggregations(
        tables: list,
        primary_table,
        table_join_cols: dict,
) -> list:
    """
    Auto-generate aggregation configs for related tables when none are
    configured in the DB.

    For each related (non-primary) table:
      1. Parse columns_metadata
      2. Skip ID/key columns (columns ending in _ID, or marked is_potential_key)
      3. Numeric columns (int64, float64) â†’ ["sum", "mean", "max"]
      4. Categorical columns (str/object)  â†’ ["nunique"]
      5. group_by = the join column for this table
      6. prefix   = TABLE_NAME_ (uppercased, shortened)

    Limits to top ~8 most useful columns per table to avoid explosion.
    """

    # Columns to always skip (by pattern)
    SKIP_PATTERNS = {"_id", "sk_id", "index"}

    # Dtype â†’ agg functions mapping
    NUMERIC_DTYPES = {"int64", "float64", "int32", "float32", "number"}
    CATEGORICAL_DTYPES = {"str", "object", "string", "category"}

    # Numeric agg functions
    NUMERIC_FUNCS = ["sum", "mean", "max"]
    CATEGORICAL_FUNCS = ["nunique"]

    # Max columns to aggregate per table
    MAX_FEATURES_PER_TABLE = 10

    agg_configs = []

    for t in tables:
        # Skip primary table â€” we don't aggregate it
        if t.id == primary_table.id:
            continue

        # Parse columns metadata
        cols = []
        try:
            raw = t.columns_metadata
            cols = json.loads(raw) if isinstance(raw, str) else (raw or [])
        except (json.JSONDecodeError, TypeError):
            continue

        if not cols:
            continue

        # Determine the group_by column for this table
        group_by = table_join_cols.get(t.id)
        if not group_by:
            continue  # Can't aggregate without a join column

        # Build features dict
        features = {}
        numeric_count = 0
        cat_count = 0

        for c in cols:
            col_name = c.get("name", "")
            dtype = (c.get("dtype") or "").lower()
            is_key = c.get("is_potential_key", False)

            # Skip ID/key columns
            if is_key:
                continue
            if any(pat in col_name.lower() for pat in SKIP_PATTERNS):
                continue

            # Skip columns that are 100% null
            null_pct = c.get("null_percentage", 0) or 0
            if null_pct >= 100:
                continue

            # Skip single-value columns (no variance)
            unique_count = c.get("unique_count", 0) or 0
            if unique_count <= 1:
                continue

            # Determine aggregation functions by dtype
            if dtype in NUMERIC_DTYPES:
                if numeric_count < MAX_FEATURES_PER_TABLE:
                    features[col_name] = NUMERIC_FUNCS.copy()
                    numeric_count += 1
            elif dtype in CATEGORICAL_DTYPES:
                if cat_count < 3:  # Limit categorical to avoid explosion
                    features[col_name] = CATEGORICAL_FUNCS.copy()
                    cat_count += 1

        if not features:
            continue

        # Build prefix from table name
        # e.g. "bureau" â†’ "BUREAU_", "credit_card_balance" â†’ "CC_BAL_"
        prefix = _make_prefix(t.table_name)

        agg_configs.append({
            "table": t.table_name,
            "group_by": group_by,
            "prefix": prefix,
            "features": features,
        })

    return agg_configs


def _make_prefix(table_name: str) -> str:
    """
    Generate a short uppercase prefix from a table name.

    Examples:
      "bureau"                 â†’ "BUREAU_"
      "previous_application"   â†’ "PREV_APP_"
      "credit_card_balance"    â†’ "CC_BAL_"
      "POS_CASH_balance"       â†’ "POS_CASH_"
      "installments_payments"  â†’ "INST_PAY_"
    """
    # Common abbreviation patterns
    ABBREVS = {
        "bureau": "BUREAU",
        "previous_application": "PREV_APP",
        "credit_card_balance": "CC_BAL",
        "pos_cash_balance": "POS_CASH",
        "installments_payments": "INST_PAY",
        "application_train": "APP",
        "application_test": "APP_TEST",
    }

    lower = table_name.lower()
    if lower in ABBREVS:
        return ABBREVS[lower] + "_"

    # Fallback: take first 3 chars of each word, uppercase
    parts = re.split(r"[_\s]+", table_name)
    if len(parts) == 1:
        return table_name.upper()[:8] + "_"
    else:
        short = "_".join(p.upper()[:4] for p in parts[:3])
        return short + "_"


def _build_data_loading_from_collection(
        db: Session,
        collection_id: str,
        file_path: str,
) -> dict:
    """
    Fetch collection metadata from DB and build the data_loading dict
    that will be injected into parameters.yml.

    Maps:
      collection.tables         â†’ data_loading.tables
      collection.relationships  â†’ data_loading.joins
      collection.aggregations   â†’ data_loading.aggregations
      collection.target_column  â†’ data_loading.target_column
    """

    # â”€â”€ Fetch from DB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    coll = db.query(DatasetCollection).filter(
        DatasetCollection.id == collection_id
    ).first()
    if not coll:
        raise HTTPException(status_code=404, detail=f"Collection {collection_id} not found")

    tables = db.query(CollectionTable).filter(
        CollectionTable.collection_id == collection_id
    ).order_by(CollectionTable.sort_order).all()

    relationships = db.query(TableRelationship).filter(
        TableRelationship.collection_id == collection_id
    ).all()

    aggregations = db.query(TableAggregation).filter(
        TableAggregation.collection_id == collection_id
    ).all()

    if not tables:
        raise HTTPException(status_code=400, detail="Collection has no tables")

    # â”€â”€ Build idâ†’name lookup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    id_to_name = {t.id: t.table_name for t in tables}

    # â”€â”€ Find primary table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    primary = next((t for t in tables if t.role == "primary"), None)
    if not primary:
        primary = next((t for t in tables if t.id == coll.primary_table_id), tables[0])

    # â”€â”€ Derive data_directory from file_path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # e.g. "data/01_raw/proj123/m1/application_train.csv" â†’ "data/01_raw/proj123/m1/"
    data_directory = str(Path(file_path).parent) + "/"

    # â”€â”€ Build table_id â†’ join_column from relationships â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    table_join_cols = {}
    for rel in relationships:
        if rel.left_table_id not in table_join_cols:
            table_join_cols[rel.left_table_id] = rel.left_column
        if rel.right_table_id not in table_join_cols:
            table_join_cols[rel.right_table_id] = rel.right_column

    # Fallback: scan columns_metadata for potential key columns
    for t in tables:
        if t.id not in table_join_cols and t.columns_metadata:
            try:
                cols = json.loads(t.columns_metadata) if isinstance(t.columns_metadata, str) else t.columns_metadata
                for c in (cols or []):
                    if c.get("is_potential_key"):
                        table_join_cols[t.id] = c["name"]
                        break
            except (json.JSONDecodeError, TypeError):
                pass

    # â”€â”€ STEP 3: Build tables list â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    tables_config = []
    for t in tables:
        id_col = table_join_cols.get(t.id, "ID")
        tables_config.append({
            "name": t.table_name,
            "filepath": t.file_name or (Path(t.file_path).name if t.file_path else "unknown.csv"),
            "id_column": id_col,
        })

    # â”€â”€ STEP 4: Build aggregations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    agg_config = []

    if aggregations:
        # â”€â”€ 4a: Use DB-configured aggregations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for agg in aggregations:
            source_name = id_to_name.get(agg.source_table_id, "unknown")

            features_raw = []
            try:
                features_raw = json.loads(agg.features) if isinstance(agg.features, str) else (agg.features or [])
            except (json.JSONDecodeError, TypeError):
                pass

            features_dict = {}
            for f in features_raw:
                col_name = f.get("column", "")
                funcs = f.get("functions", [])
                if col_name and funcs:
                    features_dict[col_name] = funcs

            agg_config.append({
                "table": source_name,
                "group_by": agg.group_by_column,
                "prefix": agg.column_prefix or f"{source_name.upper()}_",
                "features": features_dict,
            })
    else:
        # â”€â”€ 4b: Auto-generate aggregations from column metadata â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # When no aggregations are configured in the wizard, generate
        # sensible defaults for every related (non-primary) table:
        #   - numeric columns  â†’ ["sum", "mean", "max"]
        #   - categorical cols â†’ ["nunique"]
        #   - skip ID/key columns
        logger.info("ğŸ“Š No aggregations in DB â€” auto-generating from column metadata")
        agg_config = _auto_generate_aggregations(tables, primary, table_join_cols)

    # â”€â”€ STEP 5: Build joins from relationships â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    joins_config = []
    for rel in relationships:
        left_name = id_to_name.get(rel.left_table_id, "unknown")
        right_name = id_to_name.get(rel.right_table_id, "unknown")
        joins_config.append({
            "left_table": left_name,
            "right_table": right_name,
            "left_on": rel.left_column,
            "right_on": rel.right_column,
            "how": rel.join_type or "left",
        })

    # â”€â”€ Assemble data_loading â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    target = coll.target_column or "TARGET"

    data_loading = {
        "mode": "multi",
        "data_directory": data_directory,
        "main_table": primary.table_name,
        "target_column": target,
        "tables": tables_config,
    }

    if agg_config:
        data_loading["aggregations"] = agg_config

    if joins_config:
        data_loading["joins"] = joins_config

    data_loading["test_size"] = 0.2
    data_loading["random_state"] = 42
    data_loading["stratify"] = True

    logger.info(
        f"âœ… Built data_loading from collection: "
        f"tables={len(tables_config)}, joins={len(joins_config)}, "
        f"aggregations={len(agg_config)}, main_table={primary.table_name}"
    )

    return data_loading, target, data_directory


def _patch_parameters_for_multi_table(
        params_path: Path,
        data_loading: dict,
        target_column: str,
        data_directory: str,
) -> None:
    """
    Read existing parameters.yml, replace/inject the data_loading section
    with multi-table config, update related top-level keys.
    """
    with open(params_path, "r", encoding="utf-8") as f:
        params = yaml.safe_load(f) or {}

    # â”€â”€ Replace data_loading section â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    params["data_loading"] = data_loading

    # â”€â”€ Update top-level keys â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    params["target_column"] = target_column
    params["data_path"] = data_directory

    # â”€â”€ Adjust feature_engineering for multi-table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    fe = params.get("feature_engineering", {})
    fe["max_features_allowed"] = 500
    id_kw = fe.get("id_keywords", [])
    if "sk_id" not in id_kw:
        id_kw.append("sk_id")
    fe["id_keywords"] = id_kw
    params["feature_engineering"] = fe

    # â”€â”€ Adjust feature_selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    fs = params.get("feature_selection", {})
    fs["n_features"] = 30
    params["feature_selection"] = fs

    # â”€â”€ Write back â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with open(params_path, "w", encoding="utf-8") as f:
        yaml.dump(params, f, default_flow_style=False, sort_keys=False, allow_unicode=True)

    logger.info(f"âœ… Patched parameters.yml â†’ mode=multi, target={target_column}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SINGLE-TABLE: Patch parameters.yml
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _resolve_target_column(
        target_column: Optional[str],
        model_name: Optional[str],
) -> str:
    """
    Determine the target column name.

    Priority:
      1. Explicit target_column param
      2. Parsed from model_name (e.g. "sample_data__loan_approved" â†’ "loan_approved")
      3. Fallback: "TARGET"
    """
    if target_column:
        return target_column.strip()

    if model_name and "__" in model_name:
        # "sample_data__loan_approved" â†’ "loan_approved"
        parts = model_name.split("__", 1)
        if len(parts) == 2 and parts[1].strip():
            return parts[1].strip()

    return "TARGET"


def _patch_parameters_for_single_table(
        params_path: Path,
        file_path: str,
        target: str,
) -> None:
    """
    Patch parameters.yml for single-table mode:
      - data_loading.filepath  â†’ file_path
      - data_loading.target_column â†’ target
      - data_path â†’ file_path
      - target_column â†’ target
    """
    with open(params_path, "r", encoding="utf-8") as f:
        params = yaml.safe_load(f) or {}

    # Patch data_loading section
    dl = params.get("data_loading", {})
    dl["mode"] = "single"
    dl["filepath"] = file_path
    dl["target_column"] = target
    params["data_loading"] = dl

    # Patch top-level keys
    params["data_path"] = file_path
    params["target_column"] = target

    with open(params_path, "w", encoding="utf-8") as f:
        yaml.dump(params, f, default_flow_style=False, sort_keys=False, allow_unicode=True)

    logger.info(f"âœ… Patched parameters.yml: mode=single, filepath={file_path}, target={target}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DOWNLOAD ENDPOINT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@router.get("/download")
async def download_kedro_source(
        background_tasks: BackgroundTasks,
        project_id: str = Query(
            ...,
            description="Project ID for naming and directory structure",
            example="my_project",
        ),
        file_path: str = Query(
            "data/01_raw/data.csv",
            description="Raw data filepath for catalog.yml (e.g. data/01_raw/proj/data.csv)",
            example="data/01_raw/my_project/data.csv",
        ),
        collection_id: Optional[str] = Query(
            None,
            description="Collection ID for multi-table mode. When provided, fetches collection "
                        "metadata and generates multi-table parameters.yml with tables, joins, "
                        "and aggregations.",
            example="e273e5e5-0f17-4e87-84ae-1f19add9156a",
        ),
        target_column: Optional[str] = Query(
            None,
            description="Target column name for prediction. If not provided, "
                        "auto-detected from model name (e.g. 'sample_data__loan_approved' â†’ 'loan_approved').",
            example="loan_approved",
        ),
        model_name: Optional[str] = Query(
            None,
            description="Model name (used to derive target_column if not provided). "
                        "Format: dataset__target_column",
            example="sample_data__loan_approved",
        ),
        branch: str = Query(
            "main",
            description="Git branch to clone",
            example="main",
        ),
        db: Session = Depends(get_db),
):
    """
    Download Kedro ML engine source code with project-specific configuration.

    **Always patches both `catalog.yml` AND `parameters.yml`.**

    **Single table** (no collection_id):
      - `catalog.yml`: raw_data.filepath â†’ file_path
      - `parameters.yml`: data_loading.filepath â†’ file_path, target_column â†’ resolved target

    **Multi table** (collection_id provided):
      - `catalog.yml`: raw_data.filepath â†’ file_path
      - `parameters.yml`: full multi-table config from collection metadata

    Returns the repo as a `.zip` download.
    """
    # â”€â”€ Sanitize inputs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    safe_project_id = re.sub(r"[^a-zA-Z0-9_\-]", "_", project_id.strip())
    if not safe_project_id:
        raise HTTPException(status_code=400, detail="project_id is required")

    safe_file_path = file_path.strip()

    # â”€â”€ If multi-table, fetch collection data early (fail fast) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    multi_table_data = None
    if collection_id:
        multi_table_data = _build_data_loading_from_collection(
            db, collection_id.strip(), safe_file_path
        )
        logger.info(f"ğŸ“Š Multi-table mode: collection={collection_id}")

    tmp_dir = tempfile.mkdtemp(prefix="kedro_source_")

    try:
        # â”€â”€ STEP 1: Clone the repo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        clone_dir = os.path.join(tmp_dir, "kedro-ml-engine-integrated")
        logger.info(f"ğŸ“¥ Cloning {GITHUB_REPO_URL} (branch={branch})...")

        result = subprocess.run(
            ["git", "clone", "--depth", "1", "--branch", branch,
             GITHUB_REPO_URL, clone_dir],
            capture_output=True, text=True, timeout=120,
        )

        if result.returncode != 0:
            logger.error(f"Git clone failed: {result.stderr}")
            raise HTTPException(
                status_code=502,
                detail=f"Failed to clone repository: {result.stderr.strip()}",
            )
        logger.info(f"âœ… Cloned to {clone_dir}")

        # â”€â”€ STEP 2: Patch catalog.yml â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        catalog_path = Path(clone_dir) / CATALOG_REL_PATH
        if not catalog_path.exists():
            raise HTTPException(
                status_code=500,
                detail=f"catalog.yml not found at {CATALOG_REL_PATH}",
            )

        _patch_catalog(catalog_path, safe_file_path)

        # â”€â”€ STEP 3: Patch parameters.yml â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        params_path = Path(clone_dir) / PARAMS_REL_PATH
        if not params_path.exists():
            raise HTTPException(
                status_code=500,
                detail=f"parameters.yml not found at {PARAMS_REL_PATH}",
            )

        if multi_table_data:
            # Multi-table: full replacement with collection metadata
            data_loading, resolved_target, data_directory = multi_table_data
            _patch_parameters_for_multi_table(
                params_path, data_loading, resolved_target, data_directory
            )
        else:
            # Single-table: patch filepath + target_column
            resolved_target = _resolve_target_column(target_column, model_name)
            _patch_parameters_for_single_table(
                params_path, safe_file_path, resolved_target
            )

        # â”€â”€ STEP 4: Create raw data directories â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        raw_dir = Path(clone_dir) / Path(safe_file_path).parent
        raw_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"ğŸ“ Created directory: {raw_dir}")

        # â”€â”€ STEP 5: Remove .git folder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        git_dir = Path(clone_dir) / ".git"
        if git_dir.exists():
            shutil.rmtree(git_dir)

        # â”€â”€ STEP 6: Create zip â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        zip_name = f"kedro-ml-engine-{safe_project_id}"
        zip_path = os.path.join(tmp_dir, zip_name)
        shutil.make_archive(
            zip_path, "zip",
            root_dir=tmp_dir,
            base_dir="kedro-ml-engine-integrated",
        )

        zip_file = f"{zip_path}.zip"
        logger.info(f"ğŸ“¦ Created zip: {zip_file} ({os.path.getsize(zip_file)} bytes)")

        # â”€â”€ STEP 7: Return as download â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        background_tasks.add_task(shutil.rmtree, tmp_dir, True)

        return FileResponse(
            path=zip_file,
            media_type="application/zip",
            filename=f"{zip_name}.zip",
            headers={
                "Content-Disposition": f'attachment; filename="{zip_name}.zip"'
            },
        )

    except HTTPException:
        shutil.rmtree(tmp_dir, ignore_errors=True)
        raise
    except subprocess.TimeoutExpired:
        shutil.rmtree(tmp_dir, ignore_errors=True)
        raise HTTPException(status_code=504, detail="Git clone timed out (120s)")
    except Exception as e:
        shutil.rmtree(tmp_dir, ignore_errors=True)
        logger.error(f"âŒ Error in kedro source download: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))