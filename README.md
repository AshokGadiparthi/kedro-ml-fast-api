# Clean FastAPI for Pipeline Job Management

**Simple, Clean FastAPI** - No Kedro code mixed in!

This is a standalone FastAPI application that:
- âœ… Submits pipeline jobs via REST API
- âœ… Stores job records in SQLite database
- âœ… Uses Celery for background execution
- âœ… Calls external Kedro project for pipeline execution
- âœ… Minimal dependencies, maximum clarity

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FASTAPI (This Project)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ HTTP REST endpoints for job management                       â”‚
â”‚  â€¢ SQLite database for job records                              â”‚
â”‚  â€¢ Task sending to Celery workers                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ Uses Celery
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CELERY WORKER (This Project)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Background task execution                                    â”‚
â”‚  â€¢ Calls external Kedro project via CLI                         â”‚
â”‚  â€¢ Updates FastAPI database with results                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ Subprocess call
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              EXTERNAL KEDRO PROJECT (Separate Repo)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Independent Kedro pipeline project                           â”‚
â”‚  â€¢ Located at: /home/ashok/work/latest/full/kedro-engine-dynamicâ”‚
â”‚  â€¢ Called via: kedro run --pipeline <name>                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ main.py                    # FastAPI application entry point
â”œâ”€â”€ worker.py                  # Celery worker configuration
â”œâ”€â”€ celery_config.py           # Celery settings
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ jobs.db                    # SQLite database (created on first run)
â”‚
â””â”€â”€ app/                       # Application package
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ tasks.py               # Celery tasks (calls external Kedro)
    â”‚
    â”œâ”€â”€ api/                   # API endpoints
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ jobs.py            # Job submission/status endpoints
    â”‚   â”œâ”€â”€ pipelines.py       # Pipeline info endpoints
    â”‚   â””â”€â”€ health.py          # Health check endpoints
    â”‚
    â”œâ”€â”€ core/                  # Core utilities
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ job_manager.py     # Database job management
    â”‚
    â””â”€â”€ schemas/               # Pydantic models
        â”œâ”€â”€ __init__.py
        â””â”€â”€ job_schemas.py     # Job request/response models
```

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Update Kedro Project Path

Edit `app/tasks.py` and update this line:

```python
KEDRO_PROJECT_PATH = os.getenv(
    'KEDRO_PROJECT_PATH',
    '/home/ashok/work/latest/full/kedro-engine-dynamic'  # â† Change to YOUR path
)
```

Or set environment variable:

```bash
export KEDRO_PROJECT_PATH=/path/to/your/kedro/project
```

### 3. Start Redis (required for Celery)

```bash
redis-server
```

### 4. Start Celery Worker

```bash
celery -A worker worker --loglevel=info
```

### 5. Start FastAPI

In another terminal:

```bash
python main.py
```

---

## ğŸ“¡ API Usage

### 1. Submit a Job

```bash
curl -X POST http://localhost:8000/api/v1/jobs \
  -H "Content-Type: application/json" \
  -d '{
    "pipeline_name": "data_loading",
    "parameters": {}
  }'
```

Response:

```json
{
  "id": "550e8400-e29b-41d4-a716-446655440000",
  "pipeline_name": "data_loading",
  "user_id": "anonymous",
  "status": "pending",
  "parameters": {},
  "results": null,
  "error_message": null,
  "created_at": "2026-02-03T17:30:00",
  "started_at": null,
  "completed_at": null,
  "execution_time": null
}
```

### 2. Check Job Status

```bash
curl http://localhost:8000/api/v1/jobs/550e8400-e29b-41d4-a716-446655440000
```

Status values:
- `pending` - Waiting to be processed
- `running` - Pipeline executing
- `completed` - Finished successfully
- `failed` - Execution failed

### 3. List Recent Jobs

```bash
curl http://localhost:8000/api/v1/jobs?limit=10
```

### 4. Health Check

```bash
curl http://localhost:8000/api/v1/health
```

---

## ğŸ”„ How It Works

### Job Submission Flow

```
1. POST /api/v1/jobs
   â†“
2. FastAPI creates job in database (status: pending)
   â†“
3. FastAPI sends Celery task
   â†“
4. Celery task receives message from Redis
   â†“
5. Celery worker calls external Kedro project:
   $ cd /path/to/kedro && kedro run --pipeline <name>
   â†“
6. Celery updates job in database:
   - status: running â†’ completed/failed
   - results: pipeline outputs
   - error_message: if failed
   â†“
7. Client polls GET /api/v1/jobs/{id} to check status
```

---

## ğŸ”§ Configuration

### Redis Connection

Edit `celery_config.py`:

```python
broker_url = 'redis://localhost:6379/0'
result_backend = 'redis://localhost:6379/1'
```

### Kedro Project Path

Option 1: Edit `app/tasks.py`

```python
KEDRO_PROJECT_PATH = '/path/to/kedro/project'
```

Option 2: Environment variable

```bash
export KEDRO_PROJECT_PATH=/path/to/kedro/project
python main.py
```

### Database

SQLite database created automatically at project root as `jobs.db`

---

## ğŸ“Š Database Schema

```sql
CREATE TABLE jobs (
    id TEXT PRIMARY KEY,
    pipeline_name TEXT NOT NULL,
    user_id TEXT,
    status TEXT DEFAULT 'pending',  -- pending, running, completed, failed
    parameters TEXT,                 -- JSON
    results TEXT,                    -- JSON
    error_message TEXT,
    created_at TIMESTAMP,
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    execution_time REAL
)
```

---

## ğŸ› Troubleshooting

### Tasks not being executed

1. Check Redis is running:
   ```bash
   redis-cli ping
   # Should return: PONG
   ```

2. Check Celery worker is running:
   ```bash
   # Should show: celery@<hostname> ready.
   ```

3. Check Kedro project path exists:
   ```bash
   ls /path/to/kedro/project/kedro.yml
   ```

### Kedro execution fails

Check the error message in job results:

```bash
curl http://localhost:8000/api/v1/jobs/{job_id}
```

Common issues:
- Kedro project path is wrong
- Pipeline name doesn't exist
- Kedro not installed in system PATH

### Database errors

Delete old database and restart:

```bash
rm jobs.db
python main.py
```

---

## ğŸ“ Logging

View detailed logs:

- **FastAPI logs**: console output when running `python main.py`
- **Celery logs**: console output when running `celery -A worker worker`
- **Database logs**: stored in `jobs.db`

---

## âœ… Example Workflow

```bash
# 1. Start services
redis-server &
celery -A worker worker --loglevel=info &
python main.py &

# 2. Submit job
JOB_ID=$(curl -s -X POST http://localhost:8000/api/v1/jobs \
  -H "Content-Type: application/json" \
  -d '{"pipeline_name": "data_loading"}' | jq -r '.id')

echo "Job ID: $JOB_ID"

# 3. Poll status
while true; do
  STATUS=$(curl -s http://localhost:8000/api/v1/jobs/$JOB_ID | jq -r '.status')
  echo "Status: $STATUS"
  
  if [ "$STATUS" = "completed" ] || [ "$STATUS" = "failed" ]; then
    break
  fi
  
  sleep 2
done

# 4. View results
curl http://localhost:8000/api/v1/jobs/$JOB_ID | jq
```

---

## ğŸ¯ Key Features

âœ… **Separation of Concerns**
- FastAPI handles HTTP
- Celery handles background jobs
- External Kedro project handles ML pipelines

âœ… **Simple & Clean**
- Minimal code
- Clear structure
- Easy to understand and modify

âœ… **Reliable**
- SQLite for job persistence
- Redis for message passing
- Celery for distributed execution

âœ… **Scalable**
- Add more Celery workers as needed
- Scale Redis independently
- FastAPI can run on multiple servers

---

## ğŸ“š Further Reading

- FastAPI: https://fastapi.tiangolo.com/
- Celery: https://docs.celeryproject.org/
- Redis: https://redis.io/
- Kedro: https://kedro.readthedocs.io/

---

## ğŸ’¡ Tips

1. **For production**, use a proper database (PostgreSQL) instead of SQLite
2. **Add authentication** to API endpoints as needed
3. **Monitor Celery** with Flower: `pip install flower && celery -A worker flower`
4. **Scale workers** by running multiple `celery` commands on different machines
5. **Use environment variables** for configuration instead of hardcoding

---

## ğŸ“ Support

For issues:
1. Check the logs
2. Verify all services are running
3. Check Kedro project path is correct
4. Verify Redis and Celery are properly configured
